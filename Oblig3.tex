\documentclass[9pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage[norsk]{babel}
\usepackage{listings}
\DeclareUnicodeCharacter{00A0}{ }
\setcounter{section}{10}

\title{IMT2282 - Operativsystemer \\
	Oblig 2}
\author{Stian Hoel Bergseth \\
	130378}

\date{\today}


\begin{document}
\maketitle

\section{Chapter 11}
\setcounter{subsection}{5}
\subsection{Theory Questions}
\subsubsection{Hva er forskjellen på memory mapped og isolated I/O? Angi fordeler og ulemper med disse to prinsippene.}

Memory mapped IO er når IO enheter ( disker osv ) deler samme adresse bus som RAM. Dette gjør at CPUen kan bruke de samme instruksjonene for å snakke til RAM som til IO enheter, noe som senker kompleksiteten ved bruk av IO. Isolated IO er da det motsatte, hvor IO enheter har en egen adresse bus og krever ett eget sett med instruksjoner når CPUen ønsker å sende data til IO enheter. Ulempen med Memory mapped IO er at alle enheter som er koblet på det samme bus interfacet må følge samme klokkefrekvens, dette gjør at IO enheter må holde følge med CPU og RAM, noe som i praksis ikke er gjennomførbart. En av fordelen med isolated IO er da at CPU og RAM kan øke frekvensen på adresse busen utover det IO enheter klarer. Dette lar RAM og CPU "snakke" sammen i et større tempo, noe som igjen gir høyere ytelse. 
I dag har vi derfor endt opp med ( på x86 arkitekturen ) flere forskjellige bus varianter for forskjellig bruk, SATA til disker/optiske enheter, PCI express til expansjonskort ( med forskjellige størrelse for forskjellige mengder data ) og av det aller nyeste har vi fått M.2 disker som bruker PCI express interfacet.  \url{http://en.wikipedia.org/w/index.php?title=Special:CiteThisPage&page=Bus_%28computing%29&id=651785312}


%I/O enheter, som f.eks disker, har egne registere for å ta imot og sende fra seg data. Hvis man skal snakke direkte til disse, isolated I/O , krever det at man har egne instruksjoner for å sende/hente data fra disse registrene. Hvis man heller bruker memory mapped I/O så kan man bruke de samme instruksjonene for å snakke med hurtigminnet og I/O enhetene. Dette ville krevd egne funksjoner for å komunisere med I/O enheter og ville skapt ekstra overhead sammenlignet med å bare kunne aksessere enhetene som variabler i hurtigminnet. En annen fordel er at man unngår behovet for egne sikkerhetsmekanismer for I/O enheter. Med memory mapped I/O så holder det at operativsystemet ikke tilegner prosesser minneadressene som er reservert til I/O. En av ulempene ved memory mapped I/O er at både hurtigminne og I/O enhetene må kunne få med seg alle forespørrsler CPUen gjør på adresse bus'en. Her har vi et problem ettersom hurtigminnet er raskt, mens I/O enheter er vesentlig treigere. Dette har man kommet rundt ved å ha en egen dedikert bus mellom CPU og RAM, mens I/O enhetene kan enten lytte på denne bus'en( men da vil ikke I/O enhetene klare å henge med ) eller så kan memory controlleren ha et predefinert sett med adresser som den flytter ut på en I/O bus hvis CPUen ønsker å snakke til en I/O enhet.

%Isolated I/O

\subsubsection{På en harddisk, hvor mange bytes finnes som regel i en sektor? Hva er en sylinder? Hva er typisk gjennomsnittlig aksesstid for en disk i dag? Hva er overføringsraten (ca, i MB/s) mellom diskplate og buffer?}

Som regel er det 512 bytes per sektor, men med "Advanced Format" er det på nyere disker vanlig med 4096 bytes per sektor. \url{http://en.wikipedia.org/w/index.php?title=Disk_sector&oldid=630789565}
En sylinder er det vertikale settet med sektorer på tvers av platene inne i en HDD.
I følge OS kompendiumet har en gjennomsnittlig disk i dag 4.2 ms aksesstid og en overføringsrate på ca 

512 bytes per sektor, 1 sektor tar 1,4 $\mu$s. Det er ca 714 286 sektorer per sekund. Som gir oss $714 286 * 512$ bytes som igjen er 365714432 bytes eller 348MB som human-readable-bytes.sh skriptet sier. Dette gir en overføringshastighet på 348 MB/s mellom diskplate og buffer.

\subsubsection{Hva oppnår vi med å koble diskene som RAID disker? Hvordan er diskene organisert på RAID-level 1. Forklar hvordan diskene er organisert på RAID-level 5.}

Ved å koble disker sammen i RAID kan vi få flere fysiske disker til å fremstå som en logisk enhet for datamaskinen. Ved å gjøre dette kan vi få økt ytelse, økt redundans av data eller begge deler. Dette oppnår vi ved å enten stripe data mellom disker, vi deler opp en I/O operasjon over flere disker og får økt ytelse ettersom vi utnytter paralleliteten mellom diskene. Økt redundans får vi ved speiling eller paritetsfunksjonalitet. Speiling vil si at vi skriver den samme dataen til alle diskene som er satt opp for speiling(Raid 1). For eksempel ved et 2 disk Raid 1 oppsett vil både disk 1 og disk 2 lagre all data. Paritetsfunksjonaliten fungerer ved at vi striper dataen over $N - 1$ og bruker den n'te disken til å lagre paritetsinformasjonen. I praksis har man endt opp med at diskene roterer på hvilken som skal være paritetsdisken(Raid 5). For eksmpel ved et 3 disk Raid5 oppsett vil det første være disk 1 og 2 som tar imot dataen, mens disk 3 lagrer paritetsinformasjonen. Neste operasjon vil skrive dataen til disk 2 og 3 mens disk 1 lagrer paritetsinformasjonen. Og slik fortsetter det.  

\subsubsection{Hvilke fire kriterier definerer et presist interrupt?}

I kompendiumet blir disse fire kriteriene gitt: \\ 
\begin{verbatim}
1. PC lagres på et kjent sted
2. Alle instruksjoner før PC er ferdig 
3. Ingen instruksjoner etter PC er kjørt 
4. Kjent tilstand for PC-instruksjonen
\end{verbatim}

PC er forkotelsen til program counter, som er informasjon om hvor langt prossesoren har kommet i ekskiveringsløpet av en process. For å få et presist interrupt må denne lagres på et kjent sted, slik at man kan finne den igjen, alle instruksjonene opp til program counteren må være ferdig kjørt, ingen instruksjoner som kommer etter program counteren kan ha kjørt. Den instruksjonen program counteren peker på kan være ferdig eller ikke vært påbegynt, men dette må da gjøres tydelig. 

\subsubsection{Forklar forskjellen mellom HDD og SSD når det gjelder lesing, skriving/overskriving og sletting av filer. Hva er poenget med TRIM kommandoen?}

Lesing:
Ettersom HDD har en mekanisk arm som må flyttes til det stedet på platen hvor den skal lese av data er det en relativt høy latency involvert med å finne dataen som skal leses. I en SSD finnes det ingen slik mekanisk løsning, alt foregår elektronisk og det er dermed mulig å oppnå raskere respons tid. I tilegg til raskere responstid er det mulig å utnytte parallelitet mellom flere NAND brikker i samme disk og dermed få høy overføringshastighet også ( 100MB/s~ for HDD og opp mot 1000MB/s~ for M2. SSD).
Skriving:
Her har også HDD samme problemet som ved lesning, det er en relativt høy latency for å finne riktig sted å skrive, men når den først kommer dit og kan skrive sekvensielt så har den ca samme ytelse som ved lesing. Derimot ved "random-write" så stuper ytelsen ettersom store deler av tiden går med på å flytte lesearmen. SSDer derimot har ikke samme problemet og kan også her utnytte parallelitet til skriving. Det er heller ingen mekanisk latency som gjør at "random-write" lider like mye. Det som derimot skjer er at SSDen mister fordelen med parallelitet ettersom disk controlleren ikke sprer mindre writes ( 4KB ) utover flere NAND ICer. 
Overskriving:
På en SSD er det en blokk som er den minste enheten man kan slette, som består av flere pages ( pages er den minste enheten man kan skrive til/lese fra på en SSD ). Etter at en page er skrevet til så må den slettes igjen før den kan "overskrives" og hver sletting tærer på livstiden til en SSD. Dette vil si at vi ønsker minst mulig sletting/overskrivninger på en SSD. En av måtene dettes løses på er at SSDen ikke skriver over, men hvis det er mulig skriver til en ny page istedet for den som skulle overskrives. Kun hvis det ikke er mulig å skrive til en ny page vil SSDen skrive over den gamle.
En HDD har ikke de samme problemene og skriver bare over den sektoren OSet har bedt den om å skrive over.
Sletting:
Ingen av disk variantene sletter noe, bortsett fra SSDen når det er absolutt nødvendig.

TRIM:
Poenget med TRIM kommandoen er å slippe ytelsestapet som oppstår ved overskriving av en page. Ettersom disken ikke sletter filer når du faktisk ber om det så vil overskriving kunne forekomme relativt ofte. Det TRIM kommandoen gjør er å "rydde opp" på disken i bakgrunnen. Når OSet markerer en page som slettet vil TRIM kommandoen lagre blokken som pagen var en del av i en cache, slette hele blokken og skrive tilbake det som ikke skulle slettes i blokken. Dette gjør at det "alltid" er tomme pages tilgjengelig for skriving når en ny fil kommer inn og man klarer å opprettholde ytelsen ved skriving til disk.

\subsubsection{Hva betyr det at et operativsystem er tilpasset SSD disker (slik som f.eks. Windows 7 er).}

At et OS er tilpasset SSDer vil si at det støtter TRIM kommandoen og at OSet klarer å skille på HDD vs SSD slike at det ikke prøver å defragmentere SSDen. I tilegg burde filsystemet sin blokkstørrelse sammensvare med SSDen sin page størrelse og disse burde være likestilte slik at en IO operasjon som kunne blitt behandlet av en page blir spred ut over flere pages.
\subsection{Lab exercises}
\subsubsection{Skriv en PowerShell-kommandolinje som skriver ut alle prosesser som har working set større enn 10MB. Det skal skrives ut prosessnavn, prosess-ID og working set-størrelse, sortert på working set-størrelse.}
\begin{lstlisting}[breaklines]
Get-Process | Where-Object {$_.WorkingSet -gt 10MB } | Sort-Object workingSet -Descending | Format-Table -Property ProcessName,Id,WorkingSet -AutoSize
\end{lstlisting}
\subsubsection{Lag et script myprocinfo.ps1 som gir brukeren følgende meny med tilhørende funksjonalitet: \\
1 - Hvem er jeg og hva er navnet på dette scriptet? \\
2 - Hvor lenge er det siden siste boot? \\
3 - Hvor mange prosesser og tråder finnes? \\
4 - Hvor mange context switch'er fant sted siste sekund? \\
5 - Hvor stor andel av CPU-tiden ble benyttet i kernelmode og i usermode siste sekund? \\
6 - Hvor mange interrupts fant sted siste sekund? \\
9 - Avslutt dette scriptet \\
Velg en funksjon:
}

Løsning ligger her: \\
\url{https://github.com/StianHB/IMT2282/blob/master/myprocinfo.ps1}
 
\section{Chapter 12}
\setcounter{subsection}{2}
\subsection{Theory Questions}
\subsubsection{Det kreves at fire ulike betingelser alle må være oppfylt for at deadlock skal inntre. Beskriv disse kort.}
\begin{enumerate}
\item Mutual exclusion \\ 
Alle ressursene er enten låst til en prosess eller er tilgjengelig
\item Hold and wait \\
Prossesser som allerede har en ressurs men trenger flere kan spørre om å få de.
\item No preemption \\
Operativsystemet kan ikke ta tilbake ressurser fra prosseser, de må selv gi de fra seg.
\item Circular wait \\
Krever to eller flere prossesser som venter på hverandres ressurser i sirkel. A ønsker B sin, B ønsker A sin. Dette fører til at de venter på hverandre og aldri gir slipp. \\
\end{enumerate}

\subsubsection{Hva vil det si at en tilstand er “unsafe” i forbindelse med deadlock}

En unsafe tilstand sier oss at en eller flere utføringsrekkefølger av prossessene kan føre til deadlock, men det er ikke gitt at vi havner der ettersom noen av utføringsrekkefølgene kan gjøre at alle prossessene fullføres og det ikke oppstår en deadlock.

\subsubsection{Forklar kort hva en ressursgraf er og hvordan den kan benyttes til å avsløre om vi har en deadlock situasjon}

En ressursgraf er en graf hvor man har kanter som avgjør om ressurser er tildelt prosseser, om en prosses forespørr en ressurs eller om en ressurs står ledig. Ved å simulere prossesers bruk av ressurser ved hjelp av grafer kan vi finne deadlock situasjoner når det oppstår sykler i grafen. 

\subsubsection{Tanenbaum oppgave 6.2 Students working at individual PCs in a computer laboratory send their files to be printed by a server which spools the files on its hard disk. Under what conditions may a deadlock occur if the disk space for the print spool is limited? How may deadlock be avoided?}

Vi kan få en deadlock i denne situasjonen hvis det blir sendt et dokument til serveren som er større enn det er ledig diskplass. Maskinen som sender filen vil ikke få fullført sin sending og alle de andre venter på at den skal bli ferdig. Gitt at vi har prosseser som følger hold and wait prinsippet og en server som ikke preempter ut printjobber den ikke fullfører. For å unngå deadlock i denne situasjonen kan man implementere \textit{Bankers algorithm for a single resource} evt rydde opp på disken til printserveren og bruke \textit{the ostrich algorithm}.

\subsubsection{Tanenbaum oppgave 6.25 A computer has six tape drives, with n processes competing for them. Each process may need two drives. For which values of n is the system deadlock free?}

Hvis vi bruker \textit{the ostrich algorithm} så kan vi ha opp til 5 prosseser ettersom det betyr at det alltid vil være mulig for 1 prosses å fullføre det den trenger. Så ved n <= 5 vil vi ha et deadlock fritt system.

\subsubsection{A system has four processes and five allocatable resources. The current allocation and maximum needs are as follows: }
\begin{verbatim}
       Allocated  Maximum    Available
ProcA  1 0 2 1 1  1 1 2 1 3  0 0 x 1 1
ProcB  2 0 1 1 0  2 2 2 1 0
ProcC  1 1 0 1 0  2 1 3 1 0
ProcD  1 1 1 1 0  1 1 2 2 1
\end{verbatim}
What is the smallest value of x for which this is a safe state? \\
Hvis vi kun ser på ressursen i kolonnen med x så kan x være 0 ettersom det finnes en rekkefølge hvor prossesene kan kjøre ferdig ( A -> B -> C -> D ) , men hvis vi skal se på alle kolonnen spiller det ingen rolle hvilken verdi x har ettersom ProcA aldri vil få oppfylt sitt behov for ressursen i kolonne 5.

\subsection{Lab exercises}

\subsubsection{Byte konvertering} \url{https://github.com/StianHB/IMT2282/blob/master/human-readable-bytes.ps1}
\subsubsection{Nano sekund konvertering} \url{https://github.com/StianHB/IMT2282/blob/master/human-readable-ns.ps1}
\subsubsection{Effektiv minneaksesstid} \url{https://github.com/StianHB/IMT2282/blob/master/compute-memory-access.ps1}
\subsubsection{En prosess sin bruk av virtuelt og fysisk minne} \url{https://github.com/StianHB/IMT2282/blob/master/procmi.ps1}


\section{Chapter 13}
\setcounter{subsection}{5}
\subsection{Theory questions}

\subsubsection{Forklar påstanden til Popek og Goldberg fra 1974: A machine is virtualizable only if the sensitive instructions are a subset of the privileged instructions.}

Det Popek og Goldberg mente var at det kun er mulig å virtualisere maskiner hvis de sensetive instruksjoner( instruksjoner som må utføres i kernel mode ) er et subset av de priviligerte instruksjonene. Dvs at alle sensetive instruksjoner fører til en trap(interrupt) og at VMM ( Virtual machine manager/Hypervisor ) tar seg av instruksjonen på en tilfredstillende måte.

\subsubsection{Tanenbaum oppgave 7.16 VMware does binary translation one basic block at a time, then it executes the block and starts translating the next one. Could it translate the entire program in advance and then execute it? If so, what are the advantages and disadvantages of each technique?}

I teorien kan det være mulig å binæroversette et helt program på forhånd, men det vil kreve at man finner alle veiene igjennom kodebasen og oversetter de. Det er ikke gitt at man finner alle veiene igjennom koden med en binæroversetteren. Om man klarer dette så er det en liten ytelsesfortjeneste ved førstegangs ekskvering, etter det blir alle oversetingen cachet og vil kunne gi ytelse opp mot ikke-virtualisert kode. Så ulempen med forhånds oversettning er at man ikke nødvendigvis finner alle steder man burde ha oversatt så man må uansett kjøre en binæroversetter under kjøringen av programmet. Ulempen med dynamisk binæroversettning er ytelsestapet ved første gangs ekskevering.

\subsubsection{Forklar hvordan datamaskin arkitekturens beskyttelsesringer(protectionrings) benyttes ved virtualisering når virtualiseringsteknikken er binæroversetting (VMware’s teknologi).}

Før x86 platformen fikk hardware støttet virtualisering benyttet man seg av annen teknologi som eksisterte på x86 platformen. Blant annet var det 4 beskyttelsesringer som var tilgjengelig, men bare to av de var i bruk ( Ring 0 til kernel space, ring 3 til user space ). Det man da gjorde var å la gjeste operativsystemet bruke ring 1 som sitt kernel space. Dette lot hypervisoren kontrollere kernel spacet til gjeste operativsystemet. Ulempen var ved type to hypervisorer som i utgangspunktet ikke hadde mulighet til å tilby ring 0 funksjonalitet til ring 1 ettersom det kjørte i ring 3, som en user space applikasjon på host operativsystemet. Dette løste man ved hjelp av en kernelmodul til hypervisoren som lot den få kjøre kode i ring 0 når den hadde behov for å virtualisere ring 0 for gjeste operativsystemet.

\subsubsection{Hva karakteriserer en applikasjon som vil være utfordrende/problematisk å kjøre på en virtuell maskin?.}

Applikasjoner som krever mye IO, f.eks et database system, vil kunne være utfordrene å virtualisere. Applikasjoner som trenger direkte kontroll over maskinvare, som f.eks spill som ønsker kontroll over grafikkort, kan også være utfordrene. 

\subsubsection{Forklar kort fordeler og ulemper med shadowpage tables i forhold til nested/extended page tables.}

Shadow page tables er en software implementasjon for å kunne virtualisere minnebruk for guestOS og fungerer jevnt over bra. Det finnes noen edge/corner cases hvor kostnaden på å bruke shadow page tables er dyre, som f.eks ved virtualisering av applikasjoner som bruker mye minne. Nested/extended page tables er en hardware implementasjon som tillater virtualisering av minne til guestOS. Denne implementasjonen er på papiret ikke like rask som SPT, men man kan løse det ved å "kaste" raskere hardware på problemet. 

\subsubsection{Forklar kort hvordan IOMMU kan være nyttig hardwarestøtte for virtualisering.}

Med IOMMU kan man gi guestOS direkte tilgang til en I/O enhet og la mappingen gjøre at guestOS direkte kan bruke og få tilbakemelding(interrupts) fra IO enheter.


\subsection{Lab exercises}



\subsubsection{Informasjon om deler av filsystemet.}
\begin{verbatim}
Skriv et script fsinfo.ps1 som tar en directory som argument og skriver ut
• Hvor stor del av partisjonen directorien befinner seg på som er full
• Hvor mange filer (utelat alle directories!) finnes i directorien 
  (inkl subdirectorier), gjennomsnittlig filstørrelse, 
  og full path og størrelse til den største filen 
Eksempel kjøring:

$ fsinfo.ps1 cf3
Partisjonen cf3 befinner seg på er 81.4456135970535% full Det finnes 4 filer.
Den største er C:\Users\erikh\scripts\cf3\helloworld2.cf som er 302B stor.
Gjennomsnittlig filstørrelse er 245.25B.

$ pwd
Path
----
C:\Users\erikh\scripts
$ fsinfo.ps1 $(pwd)
Partisjonen C:\Users\erikh\scripts befinner seg på er 81.4456135970535% full
Det finnes 479 filer.
Den største er C:\Users\erikh\scripts\Lessmsierables-20050611 \LessMSIerables\wix.dll som er 992KB stor.
Gjennomsnittlig filstørrelse er 12.4778509916493KB.
$
\end{verbatim}

Løsningen ligger her:\\
\url{https://github.com/StianHB/IMT2282/blob/master/fnamecheck.ps1}

\subsubsection{Regulære uttrykk anvendt på filsystemet.}
Skriv et script fnamecheck.ps1 som tar en directory som argument og skriver ut alle filnavn (inkl alle subdirectories) som inneholder norske tegn og/eller mellomrom i filnavnet.

Løsningen ligger her:\\
\url{https://github.com/StianHB/IMT2282/blob/master/fsinfo.ps1}

\end{document}
